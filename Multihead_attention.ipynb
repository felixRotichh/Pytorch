{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSSMG7Mx9JoRJIWbzPUKV1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PaW_SfZ_Pg3s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads) == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(context_length, context_length)))\n",
        "\n",
        "        def forward(self, x):\n",
        "            b, num_tokens, d_in = x.shape\n",
        "            keys = self.W_key(x)\n",
        "            queries = self.W_query(x)\n",
        "            values = self.W_value(x)\n",
        "\n",
        "            keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "            values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "            queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "            keys = keys.transpose(1, 2)\n",
        "            values = values.transpose(1, 2)\n",
        "            queries = queries.transpose(1, 2)\n",
        "\n",
        "            attn_scores = queries @ keys.transpose(2,3)\n",
        "            mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "            attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "            attn_weights = torch.softmax(\n",
        "                attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "            context_vec = (attn_weights @ values).transpose(1,2)\n",
        "            context_vec = context_vec.contiguous().view(\n",
        "                b, num_tokens, self.d_out\n",
        "            )\n",
        "            context_vec = self.out_proj(context_vec)\n",
        "            return context_vec\n",
        "\n"
      ],
      "metadata": {
        "id": "pRwm7XzwTP9h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "za-JEiAhoY5G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}